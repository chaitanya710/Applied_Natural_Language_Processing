{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "term_frequency.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFHJZOAoRxp8",
        "outputId": "c6e92016-0a01-4a13-e670-030f368209e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "little 597\n",
            "said 453\n",
            "came 191\n",
            "one 183\n",
            "could 158\n",
            "king 141\n",
            "went 122\n",
            "would 112\n",
            "great 110\n",
            "day 107\n",
            "little 0.1618763557483731\n",
            "said 0.12283080260303687\n",
            "came 0.05178958785249458\n",
            "one 0.04962039045553145\n",
            "could 0.042841648590021694\n",
            "king 0.038232104121475055\n",
            "went 0.03308026030368764\n",
            "would 0.03036876355748373\n",
            "great 0.02982646420824295\n",
            "day 0.02901301518438178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#read corpus \n",
        "words = nltk.Text(nltk.corpus.gutenberg.words('bryant-stories.txt'))\n",
        "#convert to lower case \n",
        "words = [word.lower() for word in words if word.isalpha()]\n",
        "words = [word.lower() for word in words if word not in stop_words]\n",
        "\n",
        "f_dist = FreqDist(words)\n",
        "\n",
        "#print frequency distribution, raw word count \n",
        "for x,v in f_dist.most_common(10):\n",
        "  print(x,v)\n",
        "#print term frequency adjusted to document length\n",
        "for x,v in f_dist.most_common(10):\n",
        "  print(x,v/len(f_dist))"
      ]
    }
  ]
}